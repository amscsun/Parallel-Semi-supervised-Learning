{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Parallel Semi-supervised Learning\n",
    "This is an outline of the research effort on our project. It contains the trend of current research, recent development and previous works that serve as foundations of our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to answer:\n",
    "In a graph based semi-supervised learning application, the problem is usually formulated as a regularization on the smoothness and fitness of prediction function. The objective usually has two terms\n",
    "$$ J(f) = ||F||^2_\\mathcal{G} + \\frac{\\mu}{2} ||F-Y||^p   $$\n",
    "The first term represents function smoothness over graph $\\mathcal{G}$ and the second terms measures the fitness of prediction function to the given labels. When making predictions, we want to find a $f$ that minimize the above objective function. Then, there are several questions we must answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The type of Regularization Function\n",
    "What is proposed in zhu03 is a quadratic loss measure on both smoothness and fitness terms. This is known in EE community as Energy Minimization and has been a widely studied problem. The objective function can be expressed as\n",
    "$$  J(f) = \\frac{1}{2} f^T L f + \\frac{\\mu}{2} (f-y)^T I_l (f-y)  $$\n",
    "Where $L = D-W$ is the graph laplacian, and $I_l$ is a diagonal matrix with its diagonal elements being indicator of labeling. This quadratic loss function has an analytical form of derivative, and therefore the prediciton function $f$ can be solved exactly using standard linear algebra methods.\n",
    "$$  f = (L + I_l)^{-1} I_ly $$\n",
    "Since $L$ is SPD, we know the matrix $L+I_l$ must be invertible.\n",
    "Other forms of regularization has also been proposed to fit different application domains. Most of them belong to the $l_p$ laplacian regularized family of functions. The general form would be\n",
    "$$ J(f) = \\sum\\limits_{i,j} w_{ij}|| \\frac{f_i}{D_i} - \\frac{f_j}{D_j} ||^p + \\sum\\limits_{i=1}^l || f_i - y_i ||^p $$\n",
    "Optimization of such objective functions is still an open problem. The computational complexity is of key interest to us. Read through those papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Graph Construction\n",
    "The most popular method for constructing the similarity graph $W$ is through the gaussian kernel. If $x_i$ is the feature of node $i$, then $w_{ij}$ can be computed as\n",
    "$$ w_{ij} = e^{-\\frac{(x_i-x_j)^2}{2\\sigma}} $$\n",
    "But this leads to a dense graph which usually requires $O(n^3)$ computation complexity when making predictions. And because of the locality effect of label diffusion, many efforts has been proposed to parsify $W$. Those include\n",
    "* $\\epsilon$ graph, compute the matrix $W$ using the kernel function, then set all edges with values smaller than $\\epsilon$ to be 0. This approach gives a sparse graph, but do not adjust well to scaling within data densities. And the choice of $\\epsilon$ can be subjective\n",
    "* $K$-nearest neighbour graph. Each node is only connected to its k nearest neighbours. This leads to a non-symmetric directed matrix. One popular choice is to connect an edge if it is connected in one direction.\n",
    "* b-matching graph. \n",
    "\n",
    "All above methods would construct sparse graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real World Applications\n",
    "1. image labeling\n",
    "2. twitter data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
